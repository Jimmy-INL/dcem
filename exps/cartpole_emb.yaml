defaults:
  - hydra/launcher: submitit

hydra:
  name: ${method.tag}
  run:
    dir: cartpole_emb/${hydra.name}
  sweep:
    dir: cartpole_emb/2019.09.09-v5
    subdir: ${hydra.job.override_dirname}
  launcher:
    class: hydra_plugins.fairtask.FAIRTaskLauncher
    params:
      # debug launching issues, set to true to run workers in the same process.
      no_workers: false
      queue: slurm
      queues:
        local:
          class: fairtask.local.LocalQueueConfig
          params:
            num_workers: 2
        slurm:
          class: fairtask_slurm.slurm.SLURMQueueConfig
          params:
            num_jobs: ${hydra.job.num_jobs}
            num_nodes_per_job: 1
            num_workers_per_node: 1
            name: ${run}
            maxtime_mins: 4320
            # partition: priority
            # comment: For ICLR rebuttal
            partition: learnfair
            # partition: scavenge
            cpus_per_worker: 10
            mem_gb_per_worker: 64
            gres: 'gpu:1'
            log_directory: ${hydra.sweep.dir}/.slurm
            output: slurm-%j.out
            error: slurm-%j.err


plan_horizon: 20
n_train_iter: 100000
n_batch: 1
save_interval: 100
device: 'cuda'
seed: 0

method: ${dcem}

full_ctrl_opts:
  n_sample: 100
  n_elite: 10
  n_iter: 30
  temp: null
  init_sigma_scale: 0.5

dcem:
  class: cartpole_emb.DCEM
  tag: dcem
  params:
    latent_size: 2
    n_hidden: 256
    lr: 5e-4
    ctrl_opts:
      n_sample: 100
      n_elite: 10
      n_iter: 10
      init_sigma: 0.5
      temp: 1.
      lb: -1.
      ub: 1.
      normalize: True

gd:
  class: cartpole_emb.GD
  tag: gd
  params:
    latent_size: 128
    n_hidden: 256
    lr: 1e-3
    inner_optim_opts:
        n_iter: 10
        lr: 1e-1

ae:
  class: cartpole_emb.AE
  tag: ae
  params:
    latent_size: 20
    lr: 1e-3
    n_hidden: 256
    ctrl_opts:
      n_sample: 100
      n_elite: 10
      n_iter: 10
      init_sigma: 1.
      temp: null
      normalize: False
